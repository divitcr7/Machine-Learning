{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7d8d08dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing needed packages\n",
    "from numpy import loadtxt\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0c4720ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing needed packages\n",
    "from numpy import loadtxt\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "93ffa47b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "Mydata = pd.read_csv('pima-indians-diabetes.csv', delimiter = \",\",  header= None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6432d42e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6</td>\n",
       "      <td>148</td>\n",
       "      <td>72</td>\n",
       "      <td>35</td>\n",
       "      <td>0</td>\n",
       "      <td>33.6</td>\n",
       "      <td>0.627</td>\n",
       "      <td>50</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>85</td>\n",
       "      <td>66</td>\n",
       "      <td>29</td>\n",
       "      <td>0</td>\n",
       "      <td>26.6</td>\n",
       "      <td>0.351</td>\n",
       "      <td>31</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8</td>\n",
       "      <td>183</td>\n",
       "      <td>64</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>23.3</td>\n",
       "      <td>0.672</td>\n",
       "      <td>32</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>89</td>\n",
       "      <td>66</td>\n",
       "      <td>23</td>\n",
       "      <td>94</td>\n",
       "      <td>28.1</td>\n",
       "      <td>0.167</td>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>137</td>\n",
       "      <td>40</td>\n",
       "      <td>35</td>\n",
       "      <td>168</td>\n",
       "      <td>43.1</td>\n",
       "      <td>2.288</td>\n",
       "      <td>33</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   0    1   2   3    4     5      6   7  8\n",
       "0  6  148  72  35    0  33.6  0.627  50  1\n",
       "1  1   85  66  29    0  26.6  0.351  31  0\n",
       "2  8  183  64   0    0  23.3  0.672  32  1\n",
       "3  1   89  66  23   94  28.1  0.167  21  0\n",
       "4  0  137  40  35  168  43.1  2.288  33  1"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(Mydata).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7bbc3cb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8th column shows wheter a person has diabetes or not  \n",
    "\n",
    "independent_features = Mydata.iloc[:,0:-1]\n",
    "dependent_features = Mydata.iloc[:,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "143c0c7f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6</td>\n",
       "      <td>148</td>\n",
       "      <td>72</td>\n",
       "      <td>35</td>\n",
       "      <td>0</td>\n",
       "      <td>33.6</td>\n",
       "      <td>0.627</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>85</td>\n",
       "      <td>66</td>\n",
       "      <td>29</td>\n",
       "      <td>0</td>\n",
       "      <td>26.6</td>\n",
       "      <td>0.351</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8</td>\n",
       "      <td>183</td>\n",
       "      <td>64</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>23.3</td>\n",
       "      <td>0.672</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>89</td>\n",
       "      <td>66</td>\n",
       "      <td>23</td>\n",
       "      <td>94</td>\n",
       "      <td>28.1</td>\n",
       "      <td>0.167</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>137</td>\n",
       "      <td>40</td>\n",
       "      <td>35</td>\n",
       "      <td>168</td>\n",
       "      <td>43.1</td>\n",
       "      <td>2.288</td>\n",
       "      <td>33</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   0    1   2   3    4     5      6   7\n",
       "0  6  148  72  35    0  33.6  0.627  50\n",
       "1  1   85  66  29    0  26.6  0.351  31\n",
       "2  8  183  64   0    0  23.3  0.672  32\n",
       "3  1   89  66  23   94  28.1  0.167  21\n",
       "4  0  137  40  35  168  43.1  2.288  33"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "independent_features.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cec964bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    1\n",
       "1    0\n",
       "2    1\n",
       "3    0\n",
       "4    1\n",
       "Name: 8, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dependent_features.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a4d96517",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(independent_features, dependent_features, test_size=0.2 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b2b42e8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 74.03% LogisticRegression\n",
      "Accuracy: 73.38% KNN \n",
      "Accuracy: 73.38% SVM \n",
      "[20:35:13] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "Accuracy: 70.78% XGB \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "D:\\Anaconda\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    }
   ],
   "source": [
    "## Running Various models\n",
    "models = []\n",
    "models.append(('LogisticRegression', LogisticRegression()))\n",
    "models.append(('KNN ' , KNeighborsClassifier() ))\n",
    "models.append(('SVM ' , SVC()))\n",
    "models.append(('XGB ' , XGBClassifier(eta = 1) )) # learning rate= 0.1 ( or ) eta , gamma = 10 \n",
    "\n",
    "# seeing time for every model that runs \n",
    "import time\n",
    "results = []     \n",
    "names = []\n",
    "scoring = 'accuracy'\n",
    "\n",
    "for name, model in models:\n",
    "    \n",
    "    #start time = time.time\n",
    "    model.fit(x_train,y_train)\n",
    "    \n",
    "    y_pred = model.predict(x_test)\n",
    "    predictions  = [round(value) for value in y_pred]\n",
    "    \n",
    "    # Evaluatae the predictions    \n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    print(\"Accuracy: %.2f%%\" % (accuracy * 100.0),name)\n",
    "    # print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6c4f9c68",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "D:\\Anaconda\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 74.03% LogisticRegression\n",
      "\n",
      "--- 0.03388261795043945 seconds ---\n",
      "Accuracy: 73.38% KNN \n",
      "\n",
      "--- 0.008975505828857422 seconds ---\n",
      "Accuracy: 73.38% SVM \n",
      "\n",
      "--- 0.027925968170166016 seconds ---\n",
      "[20:44:34] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "Accuracy: 74.68% XGB \n",
      "\n",
      "--- 0.14261817932128906 seconds ---\n"
     ]
    }
   ],
   "source": [
    "## now if i want to increase the accuracy of XG boost i will decrease the eta values\n",
    "## Running Various models\n",
    "models = []\n",
    "models.append(('LogisticRegression', LogisticRegression()))\n",
    "models.append(('KNN ' , KNeighborsClassifier() ))\n",
    "models.append(('SVM ' , SVC()))\n",
    "models.append(('XGB ' , XGBClassifier(eta = 0.0301) )) # learning rate= 0.1 ( or ) eta , gamma = 10 \n",
    "\n",
    "# seeing time for every model that runs \n",
    "import time\n",
    "results = []     \n",
    "names = []\n",
    "scoring = 'accuracy'\n",
    "\n",
    "for name, model in models:\n",
    "    \n",
    "    start_time = time.time()\n",
    "    model.fit(x_train,y_train)\n",
    "    \n",
    "    y_pred = model.predict(x_test)\n",
    "    predictions  = [round(value) for value in y_pred]\n",
    "    \n",
    "    # Evaluatae the predictions    \n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    print(\"Accuracy: %.2f%%\" % (accuracy * 100.0),name)\n",
    "    print()\n",
    "    print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d680a903",
   "metadata": {},
   "source": [
    "The power Of Xg Boost is that im able to produce better accuracy just by small hyperparameter tuning this is the power of the XGBOost Modl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c819e46",
   "metadata": {},
   "source": [
    "Another parameter is gamma, gammma by default is zero, The Job gamma is that it controls the overfitting of the model , if we increase gamma value by eg. 10, then the trees created in the XgBoost will not cross the certain limit 10, the advantage of gamma is that we are controlling the overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e4e82c4f",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 74.03% LogisticRegression\n",
      "\n",
      "--- 0.0329442024230957 seconds ---\n",
      "Accuracy: 73.38% KNN \n",
      "\n",
      "--- 0.007947683334350586 seconds ---\n",
      "Accuracy: 73.38% SVM \n",
      "\n",
      "--- 0.017950057983398438 seconds ---\n",
      "Accuracy: 75.32% XGB \n",
      "\n",
      "--- 0.13663482666015625 seconds ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "D:\\Anaconda\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    }
   ],
   "source": [
    "## now if i want to increase the accuracy of XG boost i will decrease the eta values\n",
    "## Running Various models\n",
    "models = []\n",
    "models.append(('LogisticRegression', LogisticRegression()))\n",
    "models.append(('KNN ' , KNeighborsClassifier() ))\n",
    "models.append(('SVM ' , SVC()))\n",
    "# eval metric will remove the unwanted warning \n",
    "models.append(('XGB ' , XGBClassifier(eta = 0.0301, gamma = 10,eval_metric='logloss') )) # learning rate= 0.1 ( or ) eta , gamma = 10 \n",
    "\n",
    "# seeing time for every model that runs \n",
    "import time\n",
    "results = []     \n",
    "names = []\n",
    "scoring = 'accuracy'\n",
    "\n",
    "for name, model in models:\n",
    "    \n",
    "    start_time = time.time()\n",
    "    model.fit(x_train,y_train)\n",
    "    \n",
    "    y_pred = model.predict(x_test)\n",
    "    predictions  = [round(value) for value in y_pred]\n",
    "    \n",
    "    # Evaluatae the predictions    \n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    print(\"Accuracy: %.2f%%\" % (accuracy * 100.0),name)\n",
    "    print()\n",
    "    print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b8b3661",
   "metadata": {},
   "source": [
    "The power of Xgboost is that we are able to create Robust model with good accuracy just by tuning the parameters "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5203145a",
   "metadata": {},
   "source": [
    "Here you can see that XgBoost is taking a longer time this is not we expect, here our records are small we may not be able to see the performance of speed in this smaller dataset but if we do the same thing with a large dataset, we use distributed machines or a multi server enviromental machine then our XgBoost will take the advantage of Multiprocessor / multicore we will be able to see a significance shift in the speed of the XG Boost Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a850747f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
